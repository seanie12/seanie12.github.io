---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a PhD student at <a href="https://www.mlai-kaist.com/" style="color: #7289da; text-decoration: none;">Machine Learning and Artificial Intelligence (MLAI)</a> lab in KAIST, working on large language models and safety alignment. I am fortunate to be advised by Professor <a href="http://www.sungjuhwang.com/" style="color: #7289da; text-decoration: none;">Sung Ju Hwang</a> and <a href="https://juho-lee.github.io/" style="color: #7289da; text-decoration: none;">Juho Lee</a>. I also closely collaborate with <a href="https://ml.comp.nus.edu.sg/kawaguchi" style="color: #7289da; text-decoration: none;">Kenji Kawaguchi</a>. My research is graciously supported by the <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2023" style="color: #7289da; text-decoration: none;">Apple Scholars in AI Fellowship</a>. Here is my <a href="https://seanie12.github.io/assets/cv.pdf" class="link-in-list" style="color: #7289da; text-decoration: none;"> cv</a>.

During my PhD study, I interned at Mila in 2024, where I had the opportunity to work with <a href="https://yoshuabengio.org/" style="color: #7289da; text-decoration: none;">Yoshua Bengio</a>, <a href="https://minsuukim.github.io//" style="color: #7289da; text-decoration: none;">Minsu Kim</a>, <a href="https://mj10.github.io/" style="color: #7289da; text-decoration: none;">Moksh Jain</a>, and <a href="https://malkin1729.github.io/" style="color: #7289da; text-decoration: none;">Kolya Malkin</a>. In 2023, I completed an internship at Apple Cambridge, working with <a href="http://www.johannsen.com/" style="color: #7289da; text-decoration: none;">Anders Johannsen</a> and <a href="https://scholar.google.com/citations?user=51FYPYsAAAAJ" style="color: #7289da; text-decoration: none;">Jianpeng Cheng</a>. In 2022, I did remote internship at NUS, working with <a href="https://ml.comp.nus.edu.sg/kawaguchi" style="color: #7289da; text-decoration: none;">Kenji Kawaguchi</a>.


# üìñ Educations
- *2022.03 - present*, PhD. in Artificial Intelligence. Korea Advanced Institute of Science and Technology.
- *2020.03 - 2022.02*, M.S. in Artificial Intelligence. Korea Advanced Institute of Science and Technology.
- *2011.03 - 2018.02*, B.A. in Library and Information Science. Yonsei University.
<!-- - *2008.03 - 2011.02*, Hanyoung Foreign Language High School. -->



# üíª Work Experience
- *2024.01* - *2024.06*, Internship at <a href="https://mila.quebec/en/" style="color: #7289da; text-decoration: none;">Mila</a>. Host: <a href="https://yoshuabengio.org/" style="color: #7289da; text-decoration: none;">Yoshua Bengio</a>.

- *2023.06* - *2023.09*, Internship at Apple Cambridge. Host: <a href="http://www.johannsen.com/" style="color: #7289da; text-decoration: none;">Anders Johannsen</a>.
- *2022.06* - *2022.09*, Remote internship at <a href="https://ml.comp.nus.edu.sg/" style="color: #7289da; text-decoration: none;">NUS</a>. Host: <a href="https://ml.comp.nus.edu.sg/kawaguchi" style="color: #7289da; text-decoration: none;">Kenji Kawaguchi</a>.


# üìù Publications 

- <font size="4">SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models</font>
[[paper]](https://arxiv.org/abs/2502.12464) \\
**Seanie Lee\***, Dong Bok Lee\*, Dominik Wagner, Minki Kang, Haebin Seong, Tobias Bocklet, Juho Lee and Sung Ju Hwang (\*: equal contribution)  \\
<span style="color:purple">**Arxiv**</span> 2025


- <font size="4">FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA Subparameter Updates</font>
[[paper]](https://arxiv.org/abs/2503.07216) \\
Sangwoo Park, **Seanie Lee**, Byungjoo Kim and Sung Ju Hwang \\
<span style="color:purple">**Arxiv**</span> 2025

- <font size="4">Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training</font>
[[paper]](https://arxiv.org/abs/2503.18929) \\
Brian R. Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, **Seanie Lee**, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, Bhavya Kailkhura \\
<span style="color:purple">**Arxiv**</span> 2025



- <font size="4">HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models</font>
[[paper]](https://arxiv.org/abs/2410.01524) \\
**Seanie Lee\***, Haebin Seong\*, Dong Bok Lee, Minki Kang, Xiaoyin Chen, Dominik Wagner, Yoshua Bengio, Juho Lee and Sung Ju Hwang (\*: equal contribution)  \\
<span style="color:purple">**ICLR**</span> 2025



- <font size="4">Learning Diverse Attacks on Large Language Models for Robust Red-teaming and Safety Tuning</font>
[[paper]](https://arxiv.org/abs/2405.18540) \\
**Seanie Lee**, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin and Moksh Jain  \\
<span style="color:purple">**ICLR**</span> 2025




- <font size="4">Calibrated Decision-Making through LLM-Assisted Retrieval</font>
[[paper]](https://arxiv.org/abs/2411.08891) \\
Chaeyun Jang, Hyungi Lee, **Seanie Lee** and Juho Lee \\
<span style="color:purple">**arXiv**</span> 2024


- <font size="4">Optimized Speculative Sampling for GPU Hardware Accelerators</font>
[[paper]](https://arxiv.org/abs/2406.11016) \\
Dominik Wagner, **Seanie Lee**, Ilja Baumann, Philipp Seeberger, Korbinian Riedhammer and Tobias Bocklet \\
<span style="color:purple">**EMNLP**</span> 2024


- <font size="4">Drug Discovery with Dynamic Goal-aware Fragments</font>
[[paper]](https://arxiv.org/abs/2310.00841) \\
Seul Lee, **Seanie Lee**, Kenji Kawaguchi and Sung Ju Hwang \\
<span style="color:purple">**ICML**</span> 2024

- <font size="4">Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries</font>
[[paper]](https://arxiv.org/abs/2402.13043) \\
**Seanie Lee**, Jianpeng Cheng, Joris Driesen, Alexandru Coca and  Anders Johannsen \\
<span style="color:purple">**NAACL**</span> 2024

- <font size="4">Self-Supervised Dataset Distillation for Transfer Learning</font>
[[paper]](https://arxiv.org/abs/2310.06511) \\
Dong Bok Lee\*, **Seanie Lee\***, Joonho Ko, Kenji Kawaguchi, Juho Lee and Sung Ju Hwang (\*: equal contribution) \\
<span style="color:purple">**ICLR**</span> 2024


- <font size="4">DiffusionNAG: Task-guided Neural Architecture Generation with Diffusion Models</font>
[[paper]](https://arxiv.org/abs/2305.16943) \\
Sohyun Ahn\*  Hayeon Lee\*, Jaehyeong Jo, **Seanie Lee** and Sung Ju Hwang (\*: equal contribution) \\
<span style="color:purple">**ICLR**</span> 2024

- <font size="4">Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks</font>
[[paper]](https://arxiv.org/abs/2305.18395) \\
Minki Kang, **Seanie Lee**, Jinheon Baek, Kenji Kawaguchi and Sung Ju Hwang \\
<span style="color:purple">**NeurIPS**</span> 2023



- <font size="4">Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased Full Set Gradient Approximation</font>
[[paper]](https://arxiv.org/abs/2208.12401) \\
Jeffrey Willette\*,  **Seanie Lee**\*, Bruno Andreis, Kenji Kawaguchi, Juho Lee and Sung Ju Hwang (\*: equal contribution) \\
<span style="color:purple">**ICML**</span> 2023


- <font size="4">Margin-based Neural Network Watermarking</font>
[[paper]](https://proceedings.mlr.press/v202/kim23o.html) \\
Byungjoo Kim, Suyoung Lee,  **Seanie Lee**, Sooel Son and Sung Ju Hwang  \\
<span style="color:purple">**ICML**</span> 2023

- <font size="4">Self-Distillation for Further Pre-training of Transformers</font>
[[paper]](https://openreview.net/forum?id=kj6oK_Hj40) \\
 **Seanie Lee**, Minki Kang,  Juho Lee, Sung Ju Hwang and Kenji Kawaguchi  \\
<span style="color:purple">**ICLR**</span> 2023

- <font size="4">Self-Supervised Set Representation Learning for Unsupervised Meta-Learning</font>
[[paper]](https://openreview.net/forum?id=kIAx30hYi_p) \\
Dong Bok Lee\*, **Seanie Lee**\*, Kenji Kawaguchi, Yunji Kim, Jihwan Bang, Jung-Woo Ha and Sung Ju Hwang (\*: equal contribution)  \\
<span style="color:purple">**ICLR**</span> 2023

- <font size="4">Set-based Meta-Interpolation for Few-Task Meta-Learning</font>
[[paper]](https://arxiv.org/abs/2205.09990) \\
 **Seanie Lee\***, Bruno Andreis\*, Kenji Kawaguchi, Juho Lee and Sung Ju Hwang (\*: equal contribution) \\
<span style="color:purple">**NeurIPS**</span> 2022


- <font size="4">On Divergence Measures for Bayesian Pseudocoresets</font>
[[paper]](http://arxiv.org/abs/2210.06205) \\
 Balhae Kim, Jungwon Choi, **Seanie Lee**, Yoonho Lee, Jung-Woo Ha and Juho Lee \\
<span style="color:purple">**NeurIPS**</span> 2022


- <font size="4">Set Based Stochastic Subsampling</font>
[[paper]](https://arxiv.org/abs/2006.14222) \\
Bruno Andreis, **Seanie Lee**, A. Tuan Nguyen, Juho Lee, Eunho Yang and Sung Ju Hwang \\
<span style="color:purple">**ICML**</span> 2022

- <font size="4">Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning</font>
[[paper]](https://openreview.net/forum?id=ivQruZvXxtz) \\
**Seanie Lee\***, Hae Beom Lee\*, Juho Lee and Sung Ju Hwang (\*: equal contribution) \\
<span style="color:purple">**ICLR**</span> 2022

- <font size="4">Learning to Perturb Word Embeddings for Out-of-distribution QA</font>
[[paper]](https://aclanthology.org/2021.acl-long.434/) \\
**Seanie Lee\***, Minki Kang\*, Juho Lee and Sung Ju Hwang (\*: equal contribution) \\
<span style="color:purple">**ACL**</span> 2021

- <font size="4">Contrastive Learning with Adversarial Perturbations for Conditional Text Generation</font>
[[paper]](https://openreview.net/forum?id=Wga_hrCa3P3) \\
**Seanie Lee\***, Dong Bok Lee\* and Sung Ju Hwang (\*: equal contribution) \\
<span style="color:purple">**ICLR**</span> 2021


- <font size="4">Meta-GMVAE: Mixture of Gaussian VAE for Unsupervised Meta-Learning</font>
[[paper]](https://openreview.net/forum?id=wS0UFjsNYjn) \\
Dong Bok Lee, Dongchan Min, **Seanie Lee** and Sung Ju Hwang \\
<span style="color:purple">**ICLR**</span> 2021


- <font size="4">Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs</font>
[[paper]](https://aclanthology.org/2020.acl-main.20/) \\
 Dong Bok Lee\*, **Seanie Lee\***, WooTae Jeong, Donghwan Kim and Sung Ju Hwang (\*: equal contribution) \\
<span style="color:purple">**ACL**</span> 2020


- <font size="4">g2pM: A Neural Grapheme-to-Phoneme Conversion Package for Mandarin Chinese Based on a New Open Benchmark Dataset</font>
[[paper]](https://www.isca-speech.org/archive/pdfs/interspeech_2020/park20c_interspeech.pdf) \\
Kyubyong Park\* and **Seanie Lee\*** (\*: equal contribution) \\
<span style="color:purple">**INTERSPEECH**</span> 2020



# üéñ Honors and Awards
- *2023.03* A 2023 recipient of the <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2023" style="color: #7289da; text-decoration: none;">Apple Scholars in AI ML PhD fellowship</a>.
- **Google Travel Grant** for NeurIPS 2022 from Google
- *2018.12* <a href="https://github.com/naver/nlp-challenge" style="color: #7289da; text-decoration: none;">Silver Medal in NLP challenge</a>.
<!-- - *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->




# üí¨ Invited Talks
- *2023.10*, Tech. Talk, <a href="https://www.th-nuernberg.de/en/" style="color: #7289da; text-decoration: none;">Technische Hochschule N√ºrnberg Georg Simon Ohm</a>. Present <a href="https://docs.google.com/presentation/d/1s0M7g0kkl2tfiAEiX51uMoaAY7-EuEXt/edit?usp=sharing&ouid=117903268632818009810&rtpof=true&sd=true" style="color: #7289da; text-decoration: none;">Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased Full Set Gradient Approximation</a>.
- *2023.05*, Tech. Talk, Samsung SDS. Present <a href="https://docs.google.com/presentation/d/1s0M7g0kkl2tfiAEiX51uMoaAY7-EuEXt/edit?usp=sharing&ouid=117903268632818009810&rtpof=true&sd=true" style="color: #7289da; text-decoration: none;">Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased Full Set Gradient Approximation</a>.
- *2021.12*, Tech. Talk, NAVER corp. Present ACL 2020 paper. 
<!-- - *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->
